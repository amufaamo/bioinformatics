---
title: 機械学習
tags:
  - 機械学習
private: false
updated_at: '2019-07-01T23:34:28+09:00'
id: 2730dcbe2818df32c7d8
organization_url_name: null
slide: false
ignorePublish: false
---
# 機械学習とはなにか
人工知能の一部。学習用データセットを使って訓練した後に、未知の例について正確に判断できるアルゴリズムの能力。

学習の種類は以下の３つ

１．教師なし学習

２．教師あり学習

３．強化学習

## 教師あり学習
### KNN（K近傍法）

あるプロットのグループの中に、一つのプロットを置く。その中で、最も近いプロットと同じグループにするのが、最近傍法。

K=３にして、近い３つのプロットをしらべる。３つのうち、2つ以上が赤だと、そのプロットも赤に属する。

```knn.py
import cv2 as cv

import numpy as np

import matplotlib.pyplot as plt


# Feature set containing (x,y) values of 25 known/training data

trainData = np.random.randint(0,100,(25,2)).astype(np.float32)


# Labels each one either Red or Blue with numbers 0 and 1

responses = np.random.randint(0,2,(25,1)).astype(np.float32)


# Take Red families and plot them

red = trainData[responses.ravel()==0]

plt.scatter(red[:,0],red[:,1],80,'r','^')


# Take Blue families and plot them

blue = trainData[responses.ravel()==1]

plt.scatter(blue[:,0],blue[:,1],80,'b','s')

plt.show()


#new commer

newcomer = np.random.randint(0,100,(1,2)).astype(np.float32)

plt.scatter(newcomer[:,0],newcomer[:,1],80,'g','o')

#おまじない

knn = cv.ml.KNearest_create()




knn.train(trainData, cv.ml.ROW_SAMPLE, responses)

ret, results, neighbours ,dist = knn.findNearest(newcomer, 4)

print( "result:  {}\n".format(results) )

print( "neighbours:  {}\n".format(neighbours) )

print( "distance:  {}\n".format(dist) )

plt.show()
```



KNNで予測する。

```yoshoku.py
#とりあえず必要そうなものはすべてインストール

import pandas as pd

import matplotlib.pyplot as plt

import cv2 as cv

import numpy as np



df = pd.read_csv('data.csv').astype(np.float32)

＃CSVをインポート。2列＋一番後ろにIdentifier（3列目、0か１）.astypeを入れないと動かない。

============================================

#必要ない。グラフを書く時に必要



df_red = df[df.iloc[:,2:3] == 0]

＃3列目（２：３と書く）が、0である行だけを抜く。



R=np.array(df_red.iloc[:,0:2].values.astype(np.float32))

＃identifierを除いて、列をすべて入れる。なぜかこの書き方にしないとうまく動かない。



df_blue = df[df.iloc[:,2:3] == 1]

#同様に

B=np.array(df_blue.iloc[:,0:2].values.astype(np.float32))



=========================================



#データセットからidentifierを除いたもの

Train=df.iloc[:,0:2].values




#iデータセットからdentifierのみ

identifier=df.iloc[:,2:3].values




#newcommerのデータ。value , astypeを入力しないとうまく動かない。

new_commer=pd.DataFrame([[200,300]]).values.astype(np.float32)




knn = cv.ml.KNearest_create()

knn.train(Train, cv.ml.ROW_SAMPLE, identifier)

#おまじない。




ret, results, neighbours ,dist = knn.findNearest(new_commer, 4)

#４でもなんでもいい。あまり多くない数字がいい。



print( "result: {}\n".format(results) )

＃結果が出てくる。
```

#### 既存のCSVデータでSupport vector machineで予測する。

```support_vector_machine.py
import pandas as pd

from sklearn import datasets, model_selection,svm,metrics

import numpy as np

import cv2 as cv

#必要なものをインストール





df = pd.read_csv('data.csv').astype(np.float32)

Train=df.iloc[:,0:2].values

#identifierを除いたもの



identifier=df.iloc[:,2:3].values

#identifier




new_commer=pd.DataFrame([[2,3]]).values.astype(np.float32)

＃予測するもの





clf = svm.SVC()

clf.fit(Train,identifier)

＃おまじない



pre = clf.predict(new_commer)

＃予測



print(pre)

＃予測をプリント

```

#### K-means


```k-means.py
import matplotlib.pyplot as plt

from sklearn import cluster

from sklearn import datasets

from sklearn.cluster import KMeans

import pandas as pd

import numpy as np

#必要なものをインストール



df = pd.read_csv('data.csv').astype(np.float32)

#dataを読み込み。astypeを入れないと動かない。

Train=df.iloc[:,0:2].values

#identifierを除いたもの



#############

#ここからクラスターの数の予測。



wcss = []

#殻にする。



for i in range(1,5):

#1,5のクラスター。多ければもっとやってもいい。

    kmeans=KMeans(n_clusters = i, init='k-means++',max_iter = 300,n_init=20,random_state=0)

#n_clustersはクラスターの数。initは初期化の方法。k-means++でよい。Max_iterは繰り返しの数。デフォルトは３００．n_init初期値選択において、異なる乱数のシードで初期の重心を選ぶ処理の実行回数。random_state乱数のシードを固定するときに使う。

    kmeans.fit(Train)

#fitさせる。

    wcss.append(kmeans.inertia_)

    #これはよくわからないが必要らしい。



#プロットする。

plt.plot(range(1, 5), wcss)

plt.title('The elbow method')

plt.xlabel('Number of clusters')

plt.ylabel('WCSS') #within cluster sum of squares

plt.show()




kmeans = KMeans(n_clusters=2,init='k-means++',max_iter=300,n_init=10,random_state=0)

#プレでぃくとしたクラスター数に基づいて、計算する。

y_kmeans=kmeans.fit_predict(Train)

#トレーニングのものを当てはめる。

print(y_kmeans)

#これがトレーニングのものを計算した結果。



new_commer=pd.DataFrame([[200,300]]).values.astype(np.float32)

＃新しい値。



new_commer_predict = kmeans.predict(new_commer)

＃これで予測する。

print(new_commer_predict)

```

#### 多次元のものをKmeansでクラスタリング化してみる

```k-means2.py
import matplotlib.pyplot as plt

from sklearn import cluster

from sklearn import datasets

from sklearn.cluster import KMeans

import pandas as pd

import numpy as np



boston = datasets.load_boston()



boston_data = boston.data



wcss = [] for i in range (1,10): kmeans = KMeans(n_clusters= i , init ='k-means++',max_iter = 300, n_init = 20, random_state=0) kmeans.fit(boston.data) wcss.append(kmeans.inertia_)



plt.plot(range(1,10),wcss) plt.title('the elbow method') plt.xlabel('Number of clusters') plt.ylabel('WCSS') plt.show()



kmeans = KMeans(n_clusters=4,max_iter =300)

kmeans_predict = kmeans.fit_predict(boston_data)

print(kmeans_predict)

```


### t-SNEとは
次元を下げるもの

#### 機械学習でt-SNEで高次元のものを低次元（2次元）で表す

```t-SNE.py
import numpy as np

from sklearn import datasets

from sklearn.manifold import TSNE

from matplotlib import pyplot as plt



boston = datasets.load_boston()

model = TSNE(n_components=2)

tsne=result = model.fit_transform(boston.data)

plt.plot(tsne_result[:,0],tsne_result[:,1],".")
```


#### 機械学習でt-SNEで高次元のものを低次元（2次元）で表し、k means法で分類し色分けをする
```t-SNE2.py
import numpy as np

from sklearn import datasets

from sklearn.manifold import TSNE

from matplotlib import pyplot as plt



boston = datasets.load_boston()

model = TSNE(n_components=2)

tsne_result = model.fit_transform(boston.data)

plt.plot(tsne_result[:,0],tsne_result[:,1],".")

＃これでプロットが出る
```


####次にKMEANSで分類する。これはtsneした後のプロットをKmeanで分類

```t-sne3.py
from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=10, max_iter = 300)

kmeans_tsne = kmeans.fit_predict(tsne_result)

plt.scatter(tsne_result[:,0],tsne_result[:,1], c = kmeans_tsne,s=5)

＃Cはカラー、Sはサイズ



#次は元データをKmeansでクラスタリング

from sklearn.cluster import KMeans

kmeans = KMeans(n_clusters=10, max_iter = 300)

kmeans_tsne = kmeans.fit_predict(boston_data)

plt.scatter(tsne_result[:,0],tsne_result[:,1], c = kmeans_tsne,s=5)
```



#### 機械学習でt-SNEで高次元のものを低次元（2次元）で表し、DBSCAN法で分類し色分けをする


DBSCAN法は、三次元以上だとうまくいかない。


```DBSCAN.py
import numpy as np

from sklearn import datasets

from sklearn.manifold import TSNE

from matplotlib import pyplot as plt




boston = datasets.load_boston()

model = TSNE(n_components=2)

tsne_result = model.fit_transform(boston.data)

plt.plot(tsne_result[:,0],tsne_result[:,1],".")



boston = datasets.load_boston()

model = TSNE(n_components=2)

tsne_result = model.fit_transform(boston.data)

plt.plot(tsne_result[:,0],tsne_result[:,1],".")

```
